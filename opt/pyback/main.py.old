import os
import json
import libvirt
import asyncio
import aiohttp # <-- NEW: Needed to fetch remote VM XML files
import lzma # <-- NEW: For decompressing .xz files
import tarfile # <-- NEW: For extracting tar.xz rootfs for LXC
import shutil # <-- NEW: For deleting LXC rootfs directories
import pty # <-- NEW: For PTY allocation for LXC console
import fcntl # <-- NEW: For non-blocking I/O on PTY
import struct # <-- NEW: For terminal size
import termios # <-- NEW: For terminal settings
import logging
import re
import subprocess
from datetime import datetime
from aiohttp import web
from xml.etree import ElementTree as ET

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
LIBVIRT_URI = 'qemu:///system'
LIBVIRT_LXC_URI = 'lxc:///'  # URI for LXC containers
DEFAULT_STORAGE_PATH = '/etc/starlight/images'
DEFAULT_POOL_NAME = 'default'
REPOSITORIES_CONFIG_PATH = '/etc/starlight/repositories.json'
VM_METADATA_PATH = '/etc/starlight/vm_metadata.json'
LXC_METADATA_PATH = '/etc/starlight/lxc_metadata.json'
UPDATE_CONFIG_PATH = '/etc/starlight/update_config.json'
VERSION_FILE_PATH = '/etc/starlight/version.json'
BACKUP_DIR = '/etc/starlight/rollback_data'
# Calculate git repository path: from /path/to/repo/opt/pyback/main.py -> /path/to/repo
GIT_REPO_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
SERVICE_NAME = 'starlight-backend'  # Name of the systemd service
NETWORK_MODE = 'bridge'  # 'bridge' for bridged networking, 'nat' for default NAT
BRIDGE_NAME = 'br0'  # Name of the actual bridge device (not libvirt network)
NAT_NETWORK_NAME = 'default'  # Name of the default NAT network

# Global dictionary to track download progress
download_progress = {}

# --- VM Metadata Functions ---

def load_vm_metadata():
    """Loads VM metadata from JSON file."""
    try:
        if not os.path.exists(VM_METADATA_PATH):
            return {}
        with open(VM_METADATA_PATH, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading VM metadata: {e}")
        return {}

def save_vm_metadata(metadata):
    """Saves VM metadata to JSON file."""
    try:
        os.makedirs(os.path.dirname(VM_METADATA_PATH), exist_ok=True)
        with open(VM_METADATA_PATH, 'w') as f:
            json.dump(metadata, f, indent=2)
        return True
    except Exception as e:
        logger.error(f"Error saving VM metadata: {e}")
        return False

def get_vm_metadata(vm_name):
    """Gets metadata for a specific VM."""
    metadata = load_vm_metadata()
    return metadata.get(vm_name, {})

def set_vm_metadata(vm_name, data):
    """Sets metadata for a specific VM."""
    metadata = load_vm_metadata()
    if vm_name not in metadata:
        metadata[vm_name] = {}
    metadata[vm_name].update(data)
    save_vm_metadata(metadata)

# --- LXC Metadata Functions ---

def load_lxc_metadata():
    """Loads LXC metadata from JSON file."""
    try:
        if not os.path.exists(LXC_METADATA_PATH):
            return {}
        with open(LXC_METADATA_PATH, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading LXC metadata: {e}")
        return {}

def save_lxc_metadata(metadata):
    """Saves LXC metadata to JSON file."""
    try:
        os.makedirs(os.path.dirname(LXC_METADATA_PATH), exist_ok=True)
        with open(LXC_METADATA_PATH, 'w') as f:
            json.dump(metadata, f, indent=2)
        return True
    except Exception as e:
        logger.error(f"Error saving LXC metadata: {e}")
        return False

def get_lxc_metadata(container_name):
    """Gets metadata for a specific LXC container."""
    metadata = load_lxc_metadata()
    return metadata.get(container_name, {})

def set_lxc_metadata(container_name, data):
    """Sets metadata for a specific LXC container."""
    metadata = load_lxc_metadata()
    if container_name not in metadata:
        metadata[container_name] = {}
    metadata[container_name].update(data)
    # Store type in metadata
    metadata[container_name]['type'] = 'lxc'
    save_lxc_metadata(metadata)

def rename_vm_metadata(old_name, new_name):
    """Renames VM in metadata."""
    metadata = load_vm_metadata()
    if old_name in metadata:
        metadata[new_name] = metadata.pop(old_name)
        save_vm_metadata(metadata)

def sanitize_vm_name(name):
    """Sanitizes VM name for filesystem use."""
    # Convert to lowercase
    name = name.lower()
    # Replace spaces and special chars with hyphens
    name = re.sub(r'[^a-z0-9-]', '-', name)
    # Remove multiple consecutive hyphens
    name = re.sub(r'-+', '-', name)
    # Remove leading/trailing hyphens
    name = name.strip('-')
    # Ensure it's not empty
    if not name:
        name = 'vm'
    return name 

# --- Libvirt Utility Functions (Unchanged) ---

def get_connection(conn_type='qemu'):
    """Connects to libvirt or returns None/raises exception.
    
    Args:
        conn_type: 'qemu' for VMs or 'lxc' for containers
    """
    try:
        uri = LIBVIRT_LXC_URI if conn_type == 'lxc' else LIBVIRT_URI
        conn = libvirt.open(uri)
        if conn is None:
            logger.error(f"Failed to open connection to {uri}")
            return None
        return conn
    except libvirt.libvirtError as e:
        logger.error(f"LIBVIRT ERROR: {e}")
        return None

def get_domain_by_name(conn, name):
    """Safely look up a domain by name."""
    try:
        return conn.lookupByName(name)
    except libvirt.libvirtError:
        return None

def get_vm_ip_by_mac(mac_address, domain_name=None):
    """
    Looks up the IP address of a VM based on its MAC address.
    For NAT networks: checks the DHCP leases file of the default libvirt NAT network.
    For bridged networks: uses ARP table and optionally libvirt DHCP leases API.
    """
    # Try libvirt DHCP leases API first (works for both NAT and some bridge setups)
    if domain_name:
        try:
            conn = get_connection()
            if conn:
                domain = get_domain_by_name(conn, domain_name)
                if domain and domain.isActive():
                    # Get DHCP leases from libvirt (works if using libvirt-managed network)
                    ifaces = domain.interfaceAddresses(libvirt.VIR_DOMAIN_INTERFACE_ADDRESSES_SRC_LEASE)
                    for iface_name, iface_info in ifaces.items():
                        if iface_info.get('addrs'):
                            for addr in iface_info['addrs']:
                                if addr.get('type') == 0:  # IPv4
                                    ip = addr.get('addr')
                                    if ip and not ip.startswith('127.'):
                                        conn.close()
                                        return ip
                conn.close()
        except Exception as e:
            logger.debug(f"Could not get IP via libvirt API: {e}")
    
    # Try ARP table (works well for bridged VMs)
    try:
        with open('/proc/net/arp', 'r') as f:
            arp_content = f.read()
        
        # ARP format: IP address, HW type, Flags, HW address, Mask, Device
        for line in arp_content.splitlines()[1:]:  # Skip header
            parts = line.split()
            if len(parts) >= 4:
                arp_mac = parts[3].lower()
                arp_ip = parts[0]
                # Check if MAC matches and it's not incomplete/invalid
                if arp_mac == mac_address.lower() and parts[2] != '0x0':
                    return arp_ip
    except Exception as e:
        logger.debug(f"Error reading ARP table: {e}")
    
    # Fallback: Try legacy leases file for NAT networks
    leases_file = '/var/lib/libvirt/dnsmasq/default.leases'
    try:
        if os.path.exists(leases_file):
            with open(leases_file, 'r') as f:
                content = f.read()
            # Leases format: expiry_time mac_address ip_address hostname client_id
            for line in content.splitlines():
                parts = line.split()
                if len(parts) >= 3 and parts[1].lower() == mac_address.lower():
                    return parts[2]  # IP address is the third part
    except Exception as e:
        logger.debug(f"Error reading DHCP leases file: {e}")
    
    return None

def get_domain_info(domain):
    """Extracts relevant info from a virDomain object."""
    try:
        info = domain.info()
        state = info[0] 
        xml_desc = domain.XMLDesc(0)
        root = ET.fromstring(xml_desc)
        uuid = root.find('uuid').text

        mac_address = None
        interface = root.find("./devices/interface[@type='network']/mac")
        if interface is not None:
            mac_address = interface.get('address')
            
        vm_ip = None
        if domain.isActive() and mac_address:
            vm_ip = get_vm_ip_by_mac(mac_address, domain.name())
            
        vnc_port = None
        graphics = root.find("./devices/graphics[@type='vnc']")
        if graphics is not None and domain.isActive():
            vnc_port_str = graphics.get('port')
            if vnc_port_str is not None and vnc_port_str != '-1':
                try:
                    vnc_port = int(vnc_port_str)
                except ValueError:
                    vnc_port = None
        
        # Get video RAM
        video = root.find("./devices/video/model")
        vram = 128  # Default
        if video is not None:
            vram_attr = video.get('vram')
            if vram_attr:
                vram = int(vram_attr) // 1024  # Convert KiB to MiB
        
        # Check for audio device
        audio = root.find("./devices/sound") is not None
        
        # Get metadata
        vm_metadata = get_vm_metadata(domain.name())

        return {
            'name': domain.name(),
            'id': domain.ID() if domain.isActive() else None,
            'uuid': uuid,
            'state': state,
            'vnc_port': vnc_port, 
            'ip_address': vm_ip, 
            'memory': info[2] / 1024, 
            'vcpus': info[3],
            'autostart': domain.autostart(),
            'vram': vram,
            'audio': audio,
            'description': vm_metadata.get('description', ''),
            'resolution': vm_metadata.get('resolution', '1920x1080'),
        }
    except libvirt.libvirtError as e:
        logger.error(f"Error getting info for domain {domain.name()}: {e}")
        return None
        
def decompress_xz_file(source_path, dest_path):
    """Decompresses an .xz file. Runs in executor to avoid blocking."""
    with lzma.open(source_path, 'rb') as compressed:
        with open(dest_path, 'wb') as decompressed:
            while True:
                chunk = compressed.read(65536)
                if not chunk:
                    break
                decompressed.write(chunk)

def extract_tar_xz_rootfs(source_path, dest_dir):
    """Extracts a .tar.xz rootfs archive to a directory. Runs in executor to avoid blocking."""
    # Ensure destination directory exists
    os.makedirs(dest_dir, exist_ok=True)
    
    try:
        # Extract the tar.xz file
        logger.info(f"Starting extraction of {source_path} to {dest_dir}")
        
        with tarfile.open(source_path, 'r:xz') as tar:
            # Get list of members for verification
            members = tar.getmembers()
            logger.info(f"Archive contains {len(members)} files/directories")
            
            # Extract all members - handle Python 3.12+ filter requirements
            try:
                # Python 3.12+ - use 'tar' filter which allows absolute links (needed for LXC rootfs)
                # This is less restrictive than 'data' but still safe for container rootfs
                tar.extractall(path=dest_dir, filter='tar')
                logger.info(f"Extracted using 'tar' filter (Python 3.12+)")
            except TypeError:
                # Older Python versions don't have the filter parameter
                tar.extractall(path=dest_dir)
                logger.info(f"Extracted without filter (Python < 3.12)")
            except Exception as filter_error:
                # If 'tar' filter fails, try fully unsafe for LXC containers
                logger.warning(f"Filter extraction failed: {filter_error}, trying unsafe extraction")
                try:
                    tar.extractall(path=dest_dir, filter='fully_trusted')
                    logger.info(f"Extracted using 'fully_trusted' filter")
                except:
                    # Last resort for older Python 3.12 versions
                    tar.extractall(path=dest_dir, filter=lambda member, path: member)
                    logger.info(f"Extracted using custom filter")
            
            logger.info(f"Successfully extracted {len(members)} files from {source_path} to {dest_dir}")
        
        # Verify extraction by checking for common directories
        expected_dirs = ['bin', 'etc', 'lib', 'sbin', 'usr']
        found_dirs = []
        for d in expected_dirs:
            check_path = os.path.join(dest_dir, d)
            if os.path.exists(check_path):
                found_dirs.append(d)
        
        logger.info(f"Extraction verification: Found directories {found_dirs} in rootfs")
        
        if not found_dirs:
            raise Exception("Extraction appears incomplete - no standard directories found in rootfs")
            
    except Exception as e:
        logger.error(f"Failed to extract tar.xz rootfs: {e}")
        raise Exception(f"Rootfs extraction failed: {e}")

def create_storage_volume(conn, name, size_gb):
    """Creates a qcow2 storage volume in the default pool."""
    try:
        pool = conn.storagePoolLookupByName(DEFAULT_POOL_NAME)
        if pool.info()[0] != libvirt.VIR_STORAGE_POOL_RUNNING:
             raise Exception(f"Storage pool '{DEFAULT_POOL_NAME}' is defined but not running. Please start it with 'sudo virsh pool-start default'.")

    except libvirt.libvirtError:
        raise Exception(f"Storage pool '{DEFAULT_POOL_NAME}' not found. Please ensure the 'default' pool is defined and running.")

    size_bytes = size_gb * 1024 * 1024 * 1024
    disk_filename = f"{name}.qcow2"
    disk_path = f"{DEFAULT_STORAGE_PATH}/{disk_filename}"
    
    # Check if a volume with this name already exists
    try:
        pool.storageVolLookupByName(disk_filename)
        raise Exception(f"Disk volume '{disk_filename}' already exists in pool.")
    except libvirt.libvirtError as e:
        if "not found" not in str(e).lower():
            raise e
            
    # Define the volume XML
    vol_xml = f"""
    <volume>
      <name>{disk_filename}</name>
      <capacity unit='bytes'>{size_bytes}</capacity>
      <allocation unit='bytes'>0</allocation>
      <target>
        <path>{disk_path}</path>
        <format type='qcow2'/>
        <permissions>
          <mode>0644</mode>
          <owner>107</owner>
          <group>107</group>
        </permissions>
      </target>
    </volume>
    """
    
    vol = pool.createXML(vol_xml, 0)
    if vol is None:
        raise Exception("Libvirt failed to create storage volume (returned None).")
    
    if not os.path.exists(disk_path):
        raise Exception(f"Disk creation failed silently! Libvirt reported success, but file not found at {disk_path}. Check permissions/SELinux on host.")
        
    return disk_path

# --- Repository Configuration Functions ---

def load_repositories_config():
    """Loads the repositories configuration from JSON file."""
    try:
        if not os.path.exists(REPOSITORIES_CONFIG_PATH):
            logger.warning(f"Repository config not found at {REPOSITORIES_CONFIG_PATH}")
            return {"repositories": []}
        
        with open(REPOSITORIES_CONFIG_PATH, 'r') as f:
            config = json.load(f)
            return config
    except Exception as e:
        logger.error(f"Error loading repositories config: {e}")
        return {"repositories": []}

def save_repositories_config(config):
    """Saves the repositories configuration to JSON file."""
    try:
        os.makedirs(os.path.dirname(REPOSITORIES_CONFIG_PATH), exist_ok=True)
        with open(REPOSITORIES_CONFIG_PATH, 'w') as f:
            json.dump(config, f, indent=2)
        return True
    except Exception as e:
        logger.error(f"Error saving repositories config: {e}")
        return False

async def fetch_repository_apps(repo_url):
    """Fetches apps from a single repository URL."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(repo_url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status != 200:
                    return None
                # GitHub raw URLs return text/plain, so read as text and parse manually
                text = await resp.text()
                data = json.loads(text)
                return data.get('apps', [])
    except Exception as e:
        logger.error(f"Error fetching apps from {repo_url}: {e}")
        return None

# --- Aiohttp API Handlers ---

async def list_vms(request):
    """Returns a list of all defined and running VMs and LXC containers."""
    vm_list = []
    
    # Get VMs from QEMU
    conn = get_connection('qemu')
    if conn:
        try:
            domains = conn.listAllDomains(libvirt.VIR_CONNECT_LIST_DOMAINS_ACTIVE | libvirt.VIR_CONNECT_LIST_DOMAINS_INACTIVE)
            
            for domain in domains:
                info = get_domain_info(domain)
                if info:
                    # Mark as VM type
                    vm_metadata = get_vm_metadata(domain.name())
                    info['type'] = vm_metadata.get('type', 'vm')
                    vm_list.append(info)
            
            conn.close()
        except Exception as e:
            logger.error(f"Error listing VMs: {e}")
    
    # Get LXC containers
    lxc_conn = get_connection('lxc')
    if lxc_conn:
        try:
            containers = lxc_conn.listAllDomains(libvirt.VIR_CONNECT_LIST_DOMAINS_ACTIVE | libvirt.VIR_CONNECT_LIST_DOMAINS_INACTIVE)
            
            for container in containers:
                info = get_domain_info(container)
                if info:
                    # Mark as LXC type
                    lxc_metadata = get_lxc_metadata(container.name())
                    info['type'] = 'lxc'
                    info['description'] = lxc_metadata.get('description', '')
                    vm_list.append(info)
            
            lxc_conn.close()
        except Exception as e:
            logger.error(f"Error listing LXC containers: {e}")
    
    if not conn and not lxc_conn:
        return web.json_response({'status': 'error', 'message': 'Could not connect to libvirt.'}, status=500)
    
    return web.json_response({'status': 'success', 'vms': vm_list})

async def create_vm(request):
    """Defines a persistent VM and starts it immediately."""
    try:
        data = await request.json()
    except json.JSONDecodeError:
        return web.json_response({'status': 'error', 'message': 'Invalid JSON body.'}, status=400)
        
    name = data.get('name')
    memory_mb = data.get('memory_mb')
    vcpus = data.get('vcpus')
    iso_path = data.get('iso_path') 
    disk_size_gb = data.get('disk_size_gb')

    if not all([name, memory_mb, vcpus, disk_size_gb]):
        return web.json_response({'status': 'error', 'message': 'Missing name, memory, vcpus, or disk_size_gb.'}, status=400)

    conn = get_connection()
    if not conn:
        return web.json_response({'status': 'error', 'message': 'Could not connect to libvirt.'}, status=500)
    
    if get_domain_by_name(conn, name):
        conn.close()
        return web.json_response({'status': 'error', 'message': f'VM named {name} already exists.'}, status=409)

    # 1. Create the virtual disk image
    try:
        disk_path = create_storage_volume(conn, name, disk_size_gb)
    except Exception as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Disk creation failed: {e}'}, status=500)
        
    # 2. Build XML Configuration
    memory_kib = memory_mb * 1024
    
    disk_config = f"""
        <disk type='file' device='disk'>
          <driver name='qemu' type='qcow2'/>
          <source file='{disk_path}'/>
          <target dev='vda' bus='virtio'/>
        </disk>
    """
    
    boot_devices = "<boot dev='hd'/>" 

    if iso_path:
        if not os.path.isfile(iso_path):
            conn.close()
            return web.json_response({'status': 'error', 'message': f"ISO file not found at path: {iso_path}."}, status=400)

        disk_config += f"""
        <disk type='file' device='cdrom'>
          <driver name='qemu' type='raw'/>
          <source file='{iso_path}'/>
          <target dev='hdc' bus='sata'/>
          <readonly/>
        </disk>
        """
        boot_devices = "<boot dev='cdrom'/><boot dev='hd'/>" 

    mac_prefix = "52:54:00" 
    import random
    mac_suffix = ":".join(["{:02x}".format(random.randint(0x00, 0xff)) for _ in range(3)])
    mac_address = f"{mac_prefix}:{mac_suffix}"

    # Configure network interface based on mode
    if NETWORK_MODE == 'bridge':
        network_interface = f"""
        <interface type='bridge'>
          <source bridge='{BRIDGE_NAME}'/>
          <model type='virtio'/>
          <mac address='{mac_address}'/>
        </interface>"""
    else:
        network_interface = f"""
        <interface type='network'>
          <source network='{NAT_NETWORK_NAME}'/>
          <model type='virtio'/>
          <mac address='{mac_address}'/>
        </interface>"""

    xml_config = f"""
    <domain type='kvm'>
      <name>{name}</name>
      <memory unit='KiB'>{memory_kib}</memory>
      <currentMemory unit='KiB'>{memory_kib}</currentMemory>
      <vcpu placement='static'>{vcpus}</vcpu>
      <os>
        <type arch='x86_64' machine='pc'>hvm</type>
        {boot_devices} 
      </os>
      <features>
        <acpi/>
        <apic/>
        <pae/>
      </features>
      <devices>
        <emulator>/usr/bin/qemu-system-x86_64</emulator>
        {disk_config} 
        {network_interface}
        <!-- VNC Graphics: port='-1' requests Libvirt to automatically assign a port (5900+) -->
        <graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/>
        <console type='pty'/>
        <input type='mouse' bus='ps2'/>
        <input type='keyboard' bus='ps2'/>
        <memballoon model='virtio'/>
      </devices>
    </domain>
    """
    
    try:
        # 3. Define and Start the domain
        domain = conn.defineXML(xml_config)
        
        if domain is None:
            raise Exception("Libvirt failed to define the domain from XML.")
        
        domain.create()
        
        conn.close()
        return web.json_response({'status': 'success', 'message': f'Domain {name} defined and started persistently with a {disk_size_gb}GB disk.'})
    
    except libvirt.libvirtError as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Libvirt operation failed: {e}'}, status=500)
    except Exception as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': str(e)}, status=500)


async def get_vm_disk_info(request):
    """Gets disk size information for a VM."""
    name = request.match_info.get('name')
    
    conn = get_connection()
    if not conn:
        return web.json_response({'status': 'error', 'message': 'Could not connect to libvirt.'}, status=500)
    
    domain = get_domain_by_name(conn, name)
    if not domain:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'VM named {name} not found.'}, status=404)
    
    try:
        # Get XML and extract disk path
        xml_desc = domain.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE)
        root = ET.fromstring(xml_desc)
        
        disk_element = root.find("./devices/disk[@type='file'][@device='disk']")
        if disk_element is None:
            conn.close()
            return web.json_response({'status': 'error', 'message': 'No disk found for this VM.'}, status=404)
        
        source_element = disk_element.find('source')
        if source_element is None:
            conn.close()
            return web.json_response({'status': 'error', 'message': 'Disk has no source path.'}, status=404)
        
        disk_path = source_element.get('file')
        if not disk_path or not os.path.exists(disk_path):
            conn.close()
            return web.json_response({'status': 'error', 'message': f'Disk file not found at {disk_path}'}, status=404)
        
        # Use qemu-img to get disk size
        process = await asyncio.create_subprocess_exec(
            'qemu-img', 'info', '--output=json', disk_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            conn.close()
            return web.json_response({'status': 'error', 'message': f'Failed to get disk info: {stderr.decode()}'}, status=500)
        
        import json as json_module
        disk_info = json_module.loads(stdout.decode())
        virtual_size_bytes = disk_info.get('virtual-size', 0)
        virtual_size_gb = virtual_size_bytes / (1024**3)
        
        conn.close()
        return web.json_response({
            'status': 'success',
            'disk_size_gb': round(virtual_size_gb, 2),
            'disk_path': disk_path,
            'format': disk_info.get('format', 'unknown')
        })
        
    except Exception as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': str(e)}, status=500)


async def update_vm_settings(request):
    """Updates VM settings (memory, vcpus, disk size, name, description, autostart, vram, audio, resolution)."""
    name = request.match_info.get('name')
    
    try:
        data = await request.json()
    except json.JSONDecodeError:
        return web.json_response({'status': 'error', 'message': 'Invalid JSON body.'}, status=400)
    
    memory_mb = data.get('memory_mb')
    vcpus = data.get('vcpus')
    disk_size_gb = data.get('disk_size_gb')
    new_name = data.get('new_name')
    description = data.get('description')
    autostart = data.get('autostart')
    vram_mb = data.get('vram_mb')
    audio_enabled = data.get('audio_enabled')
    resolution = data.get('resolution')
    
    if not any([memory_mb, vcpus, disk_size_gb, new_name, description is not None, 
                autostart is not None, vram_mb, audio_enabled is not None, resolution]):
        return web.json_response({'status': 'error', 'message': 'No settings to update.'}, status=400)
    
    conn = get_connection()
    if not conn:
        return web.json_response({'status': 'error', 'message': 'Could not connect to libvirt.'}, status=500)
    
    domain = get_domain_by_name(conn, name)
    if not domain:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'VM named {name} not found.'}, status=404)
    
    # Check if VM is running for operations that require shutdown
    is_running = domain.isActive()
    requires_shutdown = memory_mb or vcpus or disk_size_gb or new_name or vram_mb or audio_enabled is not None
    
    if is_running and requires_shutdown:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'VM {name} must be shut down before modifying these settings.'}, status=400)
    
    # Handle metadata updates (can be done while running)
    if description is not None or resolution:
        metadata_updates = {}
        if description is not None:
            metadata_updates['description'] = description
        if resolution:
            metadata_updates['resolution'] = resolution
        set_vm_metadata(name, metadata_updates)
        logger.info(f"Updated metadata for VM {name}")
    
    # Handle autostart toggle (can be done while running)
    if autostart is not None:
        try:
            if autostart:
                domain.setAutostart(1)
                logger.info(f"Enabled autostart for VM {name}")
            else:
                domain.setAutostart(0)
                logger.info(f"Disabled autostart for VM {name}")
        except libvirt.libvirtError as e:
            logger.error(f"Failed to set autostart for VM {name}: {e}")
    
    # If no XML changes needed, return success
    if not (memory_mb or vcpus or disk_size_gb or new_name or vram_mb or audio_enabled is not None):
        conn.close()
        changes = []
        if description is not None:
            changes.append('Description')
        if resolution:
            changes.append(f'Resolution: {resolution}')
        if autostart is not None:
            changes.append(f'Autostart: {"enabled" if autostart else "disabled"}')
        return web.json_response({
            'status': 'success',
            'message': f'VM {name} settings updated successfully. Changes: {", ".join(changes)}'
        })
    
    try:
        # Get current XML configuration
        xml_desc = domain.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE)
        root = ET.fromstring(xml_desc)
        
        # Handle disk resizing first (if requested)
        if disk_size_gb:
            disk_element = root.find("./devices/disk[@type='file'][@device='disk']")
            if disk_element is not None:
                source_element = disk_element.find('source')
                if source_element is not None:
                    disk_path = source_element.get('file')
                    if disk_path and os.path.exists(disk_path):
                        logger.info(f"Resizing disk {disk_path} to {disk_size_gb}GB")
                        
                        # Use qemu-img to resize the disk
                        process = await asyncio.create_subprocess_exec(
                            'qemu-img', 'resize', disk_path, f'{disk_size_gb}G',
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE
                        )
                        stdout, stderr = await process.communicate()
                        
                        if process.returncode != 0:
                            error_msg = stderr.decode() if stderr else "Unknown error"
                            # Check if it's a "shrink" error
                            if "shrink" in error_msg.lower():
                                conn.close()
                                return web.json_response({
                                    'status': 'error', 
                                    'message': 'Cannot shrink disk. Disk can only be expanded.'
                                }, status=400)
                            else:
                                conn.close()
                                return web.json_response({
                                    'status': 'error', 
                                    'message': f'Failed to resize disk: {error_msg}'
                                }, status=500)
                        
                        logger.info(f"Disk resized successfully to {disk_size_gb}GB")
        
        # Update memory if provided
        if memory_mb:
            memory_kib = memory_mb * 1024
            
            memory_element = root.find('memory')
            if memory_element is not None:
                memory_element.text = str(memory_kib)
                memory_element.set('unit', 'KiB')
            
            current_memory_element = root.find('currentMemory')
            if current_memory_element is not None:
                current_memory_element.text = str(memory_kib)
                current_memory_element.set('unit', 'KiB')
        
        # Update vCPUs if provided
        if vcpus:
            vcpu_element = root.find('vcpu')
            if vcpu_element is not None:
                vcpu_element.text = str(vcpus)
        
        # Update video RAM if provided
        if vram_mb:
            devices = root.find('devices')
            if devices is not None:
                # Find or create video device
                video = devices.find('video')
                if video is None:
                    video = ET.SubElement(devices, 'video')
                
                model = video.find('model')
                if model is None:
                    model = ET.SubElement(video, 'model')
                    model.set('type', 'qxl')  # Use QXL for better performance
                
                # VRAM is in KiB for libvirt
                model.set('vram', str(vram_mb * 1024))
                model.set('heads', '1')
                logger.info(f"Set VRAM to {vram_mb}MB for VM {name}")
        
        # Update audio device if provided
        if audio_enabled is not None:
            devices = root.find('devices')
            if devices is not None:
                # Remove existing sound devices
                for sound in devices.findall('sound'):
                    devices.remove(sound)
                
                # Add sound device if enabled
                if audio_enabled:
                    sound = ET.SubElement(devices, 'sound')
                    sound.set('model', 'ich9')  # Modern audio controller
                    logger.info(f"Enabled audio for VM {name}")
                else:
                    logger.info(f"Disabled audio for VM {name}")
        
        # Handle VM renaming FIRST (before XML changes)
        disk_renamed = False
        sanitized_new_name = None
        if new_name and new_name != name:
            # Sanitize the new name for filesystem
            sanitized_new_name = sanitize_vm_name(new_name)
            logger.info(f"Sanitized VM name: {new_name} -> {sanitized_new_name}")
            
            # Get current disk path from XML
            disk_element = root.find("./devices/disk[@type='file'][@device='disk']")
            if disk_element is not None:
                source_element = disk_element.find('source')
                if source_element is not None:
                    old_disk_path = source_element.get('file')
                    
                    if old_disk_path and os.path.exists(old_disk_path):
                        # Rename disk file FIRST
                        new_disk_path = f"{DEFAULT_STORAGE_PATH}/{sanitized_new_name}.qcow2"
                        try:
                            os.rename(old_disk_path, new_disk_path)
                            logger.info(f"Renamed disk from {old_disk_path} to {new_disk_path}")
                            disk_renamed = True
                            
                            # NOW update the disk path in XML to match the renamed file
                            source_element.set('file', new_disk_path)
                            logger.info(f"Updated disk path in XML to {new_disk_path}")
                        except Exception as e:
                            logger.error(f"Could not rename disk file: {e}")
                            conn.close()
                            return web.json_response({'status': 'error', 'message': f'Failed to rename disk file: {e}'}, status=500)
            
            # Update VM name in XML
            name_element = root.find('name')
            if name_element is not None:
                name_element.text = new_name
                logger.info(f"Updated VM name in XML from {name} to {new_name}")
        
        # Convert back to XML string
        modified_xml = ET.tostring(root, encoding='unicode')
        
        # Undefine the old domain and define with new settings
        # Use flags to handle NVRAM (UEFI) VMs properly
        try:
            # Try to undefine while keeping NVRAM (important for UEFI VMs)
            undefine_flags = (libvirt.VIR_DOMAIN_UNDEFINE_MANAGED_SAVE | 
                             libvirt.VIR_DOMAIN_UNDEFINE_SNAPSHOTS_METADATA | 
                             libvirt.VIR_DOMAIN_UNDEFINE_KEEP_NVRAM)  # Keep NVRAM when updating settings
            domain.undefineFlags(undefine_flags)
            logger.info(f"Successfully undefined VM {name} with NVRAM preservation")
        except libvirt.libvirtError as e:
            # If KEEP_NVRAM fails, the VM might not have NVRAM - try regular undefine
            logger.warning(f"Could not undefine with KEEP_NVRAM flag, trying regular undefine: {e}")
            try:
                domain.undefine()
            except libvirt.libvirtError as e2:
                # Last resort: try without any flags
                logger.error(f"Regular undefine also failed: {e2}")
                # If we renamed the disk, try to rename it back
                if disk_renamed and sanitized_new_name:
                    try:
                        os.rename(f"{DEFAULT_STORAGE_PATH}/{sanitized_new_name}.qcow2", 
                                 f"{DEFAULT_STORAGE_PATH}/{name}.qcow2")
                        logger.info(f"Rolled back disk rename")
                    except:
                        pass
                raise Exception(f"Could not undefine VM for settings update: {e2}")
        
        new_domain = conn.defineXML(modified_xml)
        
        if new_domain is None:
            raise Exception("Failed to redefine VM with new settings.")
        
        # Update metadata for renamed VM
        if new_name and new_name != name:
            rename_vm_metadata(name, new_name)
            logger.info(f"Updated metadata from {name} to {new_name}")
        
        conn.close()
        
        # Build changes list
        changes = []
        if new_name and new_name != name:
            changes.append(f'Name: {name} â†’ {new_name}')
        if description is not None:
            changes.append('Description updated')
        if memory_mb:
            changes.append(f'Memory: {memory_mb}MB')
        if vcpus:
            changes.append(f'vCPUs: {vcpus}')
        if disk_size_gb:
            changes.append(f'Disk: {disk_size_gb}GB')
        if autostart is not None:
            changes.append(f'Autostart: {"enabled" if autostart else "disabled"}')
        if vram_mb:
            changes.append(f'VRAM: {vram_mb}MB')
        if audio_enabled is not None:
            changes.append(f'Audio: {"enabled" if audio_enabled else "disabled"}')
        if resolution:
            changes.append(f'Resolution hint: {resolution}')
        
        final_name = new_name if new_name and new_name != name else name
        return web.json_response({
            'status': 'success', 
            'message': f'VM settings updated successfully. Changes: {", ".join(changes)}'
        })
        
    except libvirt.libvirtError as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Libvirt operation failed: {e}'}, status=500)
    except Exception as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': str(e)}, status=500)


async def vm_action(request):
    """Performs an action (start, stop, destroy, delete/undefine) on a VM or LXC container."""
    name = request.match_info.get('name')
    action = request.match_info.get('action')
    
    # Try to find the domain in either QEMU or LXC connections
    # First, check LXC metadata to determine type
    lxc_metadata = get_lxc_metadata(name)
    is_lxc = lxc_metadata.get('type') == 'lxc'
    
    # Try the appropriate connection first based on metadata
    conn = None
    domain = None
    conn_type = 'lxc' if is_lxc else 'qemu'
    
    conn = get_connection(conn_type)
    if conn:
        domain = get_domain_by_name(conn, name)
    
    # If not found and we checked QEMU, try LXC
    if not domain and conn_type == 'qemu':
        if conn:
            conn.close()
        conn = get_connection('lxc')
        if conn:
            domain = get_domain_by_name(conn, name)
            conn_type = 'lxc' if domain else 'qemu'
    
    # If not found and we checked LXC, try QEMU
    if not domain and conn_type == 'lxc':
        if conn:
            conn.close()
        conn = get_connection('qemu')
        if conn:
            domain = get_domain_by_name(conn, name)
            conn_type = 'qemu' if domain else 'lxc'
    
    if not conn:
        return web.json_response({'status': 'error', 'message': 'Could not connect to libvirt.'}, status=500)
    
    if not domain:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'VM/Container named {name} not found.'}, status=404)

    try:
        entity_type = "Container" if conn_type == 'lxc' else "VM"
        
        if action == 'start':
            domain.create() 
            message = f'{entity_type} {name} started.'
        elif action == 'stop':
            domain.shutdown() 
            message = f'{entity_type} {name} received graceful shutdown request.'
        elif action == 'destroy':
            domain.destroy() 
            message = f'{entity_type} {name} force stopped (destroyed).'
        elif action == 'delete':
            if domain.isActive():
                domain.destroy()
            
            message = ""
            
            # Handle storage cleanup based on type
            if conn_type == 'lxc':
                # For LXC containers, delete the rootfs directory
                rootfs_path = f"{DEFAULT_STORAGE_PATH}/{name}-rootfs"
                try:
                    if os.path.exists(rootfs_path):
                        shutil.rmtree(rootfs_path)
                        message += f' Associated rootfs directory {rootfs_path} also deleted.'
                        logger.info(f"Deleted LXC rootfs at {rootfs_path}")
                    else:
                        message += f' Note: Rootfs directory not found at {rootfs_path}.'
                except Exception as e:
                    message += f' Warning: Could not delete rootfs directory: {e}'
                    logger.warning(f"Failed to delete LXC rootfs at {rootfs_path}: {e}")
                
                # Remove from LXC metadata
                try:
                    lxc_metadata = load_lxc_metadata()
                    if name in lxc_metadata:
                        del lxc_metadata[name]
                        save_lxc_metadata(lxc_metadata)
                        logger.info(f"Removed {name} from LXC metadata")
                except Exception as e:
                    logger.warning(f"Failed to update LXC metadata: {e}")
            else:
                # For VMs, delete the qcow2 disk volume
                try:
                    pool = conn.storagePoolLookupByName(DEFAULT_POOL_NAME)
                    disk_filename = f"{name}.qcow2"
                    vol = pool.storageVolLookupByName(disk_filename)
                    if vol:
                        vol.delete(0) 
                        message += f' Associated disk {disk_filename} also deleted.'
                except libvirt.libvirtError:
                    message += f' Note: Could not find/delete disk volume for {name}.qcow2.'
            
            # Undefine the domain
            try:
                # Try to undefine with NVRAM and other managed files (for VMs)
                if conn_type == 'qemu':
                    undefine_flags = (libvirt.VIR_DOMAIN_UNDEFINE_MANAGED_SAVE | 
                                     libvirt.VIR_DOMAIN_UNDEFINE_SNAPSHOTS_METADATA | 
                                     libvirt.VIR_DOMAIN_UNDEFINE_NVRAM)
                    domain.undefineFlags(undefine_flags)
                else:
                    # For LXC, use simpler undefine
                    domain.undefine()
            except libvirt.libvirtError:
                # Fallback to regular undefine if flags not supported
                domain.undefine()
            
            message = f'{entity_type} {name} configuration permanently removed (undefined). ' + message
            
        else:
            conn.close()
            return web.json_response({'status': 'error', 'message': f'Invalid action: {action}'}, status=400)

        conn.close()
        return web.json_response({'status': 'success', 'message': message})

    except libvirt.libvirtError as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Libvirt operation failed: {e}'}, status=500)


# --- NEW HANDLER: Deploy VM from Remote XML URL ---

async def deploy_vm_from_url(request):
    """
    Fetches a VM/LXC XML definition from a remote URL, defines it in libvirt, 
    and then starts it. Optionally downloads a cloud image to use as the base disk.
    Supports both VMs and LXC containers based on 'type' field.
    """
    try:
        data = await request.json()
    except json.JSONDecodeError:
        return web.json_response({'status': 'error', 'message': 'Invalid JSON body.'}, status=400)

    # DEBUG: Log the entire request body
    logger.info(f"Deploy request body: {json.dumps(data, indent=2)}")

    xml_url = data.get('xml_url')
    vm_name = data.get('vm_name')
    disk_size_gb = data.get('disk_size_gb')
    iso_path = data.get('iso_path')  # Optional: path to ISO for installation
    cloud_image_url = data.get('cloud_image_url')  # Optional: URL to download base image
    
    # Also check for image_source structure (from repo.json format)
    if not cloud_image_url:
        image_source = data.get('image_source')
        logger.info(f"Checking image_source: {image_source}")
        if image_source and isinstance(image_source, dict):
            cloud_image_url = image_source.get('url')
            logger.info(f"Extracted URL from image_source: {cloud_image_url}")
    
    deploy_type = data.get('type', 'vm')  # 'vm' or 'lxc'
    
    # Debug logging
    logger.info(f"Deploy request received - Name: {vm_name}, Type: {deploy_type}, XML URL: {xml_url}, Image URL: {cloud_image_url}")

    if not all([xml_url, vm_name, disk_size_gb]):
        return web.json_response({'status': 'error', 'message': 'Missing XML URL, VM name, or required disk size.'}, status=400)
    
    # Warn if deploying LXC without rootfs
    if deploy_type == 'lxc' and not cloud_image_url and not iso_path:
        logger.warning(f"LXC container {vm_name} being deployed without rootfs image - it will be empty!")
    
    # Connect to appropriate libvirt instance based on type
    conn_type = 'lxc' if deploy_type == 'lxc' else 'qemu'
    conn = get_connection(conn_type)
    if not conn:
        return web.json_response({'status': 'error', 'message': f'Could not connect to libvirt ({conn_type}).'}, status=500)
    
    if get_domain_by_name(conn, vm_name):
        conn.close()
        return web.json_response({'status': 'error', 'message': f'{"Container" if deploy_type == "lxc" else "VM"} named {vm_name} already exists.'}, status=409)

    # 1. Fetch the XML from the remote URL
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(xml_url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status != 200:
                    raise Exception(f"Failed to fetch XML from {xml_url}. Status: {resp.status}")
                xml_config = await resp.text()
    except Exception as e:
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Remote XML fetch error: {e}'}, status=500)
        
    # 2. Create the disk/rootfs - either empty or download a cloud image/rootfs
    disk_path = None
    rootfs_path = None
    try:
        if cloud_image_url:
            # Initialize download progress tracking
            download_progress[vm_name] = {
                'status': 'downloading',
                'progress': 0,
                'total': 0,
                'message': 'Starting download...'
            }
            
            # Check if URL points to a compressed file
            is_tar_xz = cloud_image_url.endswith('.tar.xz')
            is_xz = cloud_image_url.endswith('.xz') and not is_tar_xz
            
            # For LXC containers with tar.xz rootfs
            if deploy_type == 'lxc' and is_tar_xz:
                rootfs_path = f"{DEFAULT_STORAGE_PATH}/{vm_name}-rootfs"
                download_path = f"{DEFAULT_STORAGE_PATH}/{vm_name}-rootfs.tar.xz"
                
                logger.info(f"Downloading LXC rootfs from {cloud_image_url} to {download_path}")
                
                # Download the rootfs tarball with progress tracking
                async with aiohttp.ClientSession() as session:
                    async with session.get(cloud_image_url, timeout=aiohttp.ClientTimeout(total=1200)) as resp:
                        if resp.status != 200:
                            raise Exception(f"Failed to download rootfs. Status: {resp.status}")
                        
                        total_size = resp.content_length or 0
                        download_progress[vm_name]['total'] = total_size
                        downloaded = 0
                        
                        # Stream download to disk
                        with open(download_path, 'wb') as f:
                            async for chunk in resp.content.iter_chunked(65536):  # 64KB chunks
                                f.write(chunk)
                                downloaded += len(chunk)
                                download_progress[vm_name]['progress'] = downloaded
                                if total_size > 0:
                                    percent = int((downloaded / total_size) * 100)
                                    download_progress[vm_name]['message'] = f'Downloading: {percent}% ({downloaded // (1024*1024)}MB / {total_size // (1024*1024)}MB)'
                
                logger.info(f"Rootfs tarball downloaded successfully.")
                
                # Extract the tar.xz rootfs
                download_progress[vm_name]['status'] = 'extracting'
                download_progress[vm_name]['message'] = 'Extracting rootfs...'
                logger.info(f"Extracting {download_path} to {rootfs_path}")
                
                # Run extraction in executor to avoid blocking
                loop = asyncio.get_event_loop()
                try:
                    await loop.run_in_executor(None, extract_tar_xz_rootfs, download_path, rootfs_path)
                except Exception as e:
                    # Cleanup on extraction failure
                    logger.error(f"Extraction failed for {vm_name}: {e}")
                    if os.path.exists(download_path):
                        os.remove(download_path)
                    if os.path.exists(rootfs_path):
                        shutil.rmtree(rootfs_path)
                    raise Exception(f"Failed to extract rootfs: {e}")
                
                # Remove tarball after successful extraction
                os.remove(download_path)
                logger.info(f"Extraction complete. Removed {download_path}")
                
                # Verify rootfs exists and has content
                if not os.path.exists(rootfs_path):
                    raise Exception(f"Rootfs extraction failed: directory not found at {rootfs_path}")
                
                # Check that rootfs has the expected structure
                required_paths = [os.path.join(rootfs_path, d) for d in ['bin', 'etc', 'sbin']]
                missing_paths = [p for p in required_paths if not os.path.exists(p)]
                if len(missing_paths) == len(required_paths):
                    raise Exception(f"Rootfs extraction appears incomplete - missing core directories")
                
                # Check for init binary location (important for LXC)
                init_paths = ['/sbin/init', '/bin/init', '/lib/systemd/systemd', '/init']
                found_init = None
                for init_path in init_paths:
                    full_init_path = os.path.join(rootfs_path, init_path.lstrip('/'))
                    if os.path.exists(full_init_path):
                        found_init = init_path
                        logger.info(f"Found init at {init_path} in rootfs")
                        break
                
                if not found_init:
                    logger.warning(f"No init found in rootfs at standard locations. Container may fail to start.")
                
                download_progress[vm_name]['status'] = 'complete'
                download_progress[vm_name]['message'] = 'Rootfs ready'
                logger.info(f"Rootfs preparation complete for {vm_name}")
                
                disk_path = rootfs_path  # Set disk_path to rootfs for XML injection
                
            # For VMs with qcow2 images
            else:
                # Download cloud image and use it as the base disk
                disk_path = f"{DEFAULT_STORAGE_PATH}/{vm_name}.qcow2"
                download_path = f"{disk_path}.xz" if is_xz else disk_path
                
                logger.info(f"Downloading cloud image from {cloud_image_url} to {download_path}")
                
                # Download the image with progress tracking
                async with aiohttp.ClientSession() as session:
                    async with session.get(cloud_image_url, timeout=aiohttp.ClientTimeout(total=1200)) as resp:
                        if resp.status != 200:
                            raise Exception(f"Failed to download cloud image. Status: {resp.status}")
                        
                        total_size = resp.content_length or 0
                        download_progress[vm_name]['total'] = total_size
                        downloaded = 0
                        
                        # Stream download to disk
                        with open(download_path, 'wb') as f:
                            async for chunk in resp.content.iter_chunked(65536):  # 64KB chunks
                                f.write(chunk)
                                downloaded += len(chunk)
                                download_progress[vm_name]['progress'] = downloaded
                                if total_size > 0:
                                    percent = int((downloaded / total_size) * 100)
                                    download_progress[vm_name]['message'] = f'Downloading: {percent}% ({downloaded // (1024*1024)}MB / {total_size // (1024*1024)}MB)'
                    
                        logger.info(f"Cloud image downloaded successfully.")
                        
                        # Decompress if needed (run in executor to avoid blocking)
                        if is_xz:
                            download_progress[vm_name]['status'] = 'decompressing'
                            download_progress[vm_name]['message'] = 'Decompressing image...'
                            logger.info(f"Decompressing {download_path} to {disk_path}")
                            
                            # Run decompression in a thread to avoid blocking the event loop
                            loop = asyncio.get_event_loop()
                            await loop.run_in_executor(None, decompress_xz_file, download_path, disk_path)
                            
                            # Remove compressed file
                            os.remove(download_path)
                            logger.info(f"Decompression complete. Removed {download_path}")
                            
                            # Verify decompressed file exists and is valid
                            if not os.path.exists(disk_path):
                                raise Exception(f"Decompression failed: output file not found at {disk_path}")
                            
                            file_size = os.path.getsize(disk_path)
                            logger.info(f"Decompressed file size: {file_size / (1024*1024):.2f} MB")
                            
                            # Check file format - might need conversion from raw to qcow2
                            download_progress[vm_name]['status'] = 'checking'
                            download_progress[vm_name]['message'] = 'Verifying image format...'
                            logger.info(f"Checking image format of {disk_path}")
                            
                            # Check what format the file is
                            check_process = await asyncio.create_subprocess_exec(
                                'qemu-img', 'info', disk_path,
                                stdout=asyncio.subprocess.PIPE,
                                stderr=asyncio.subprocess.PIPE
                            )
                            stdout, stderr = await check_process.communicate()
                            
                            if check_process.returncode == 0:
                                format_info = stdout.decode()
                                logger.info(f"Image format info: {format_info}")
                                
                                # If it's a raw image, convert to qcow2
                                if 'file format: raw' in format_info:
                                    logger.info(f"Converting raw image to qcow2 format...")
                                    download_progress[vm_name]['message'] = 'Converting to qcow2 format...'
                                    
                                    temp_path = f"{disk_path}.raw"
                                    os.rename(disk_path, temp_path)
                                    
                                    convert_process = await asyncio.create_subprocess_exec(
                                        'qemu-img', 'convert', '-f', 'raw', '-O', 'qcow2', temp_path, disk_path,
                                        stdout=asyncio.subprocess.PIPE,
                                        stderr=asyncio.subprocess.PIPE
                                    )
                                    convert_stdout, convert_stderr = await convert_process.communicate()
                                    
                                    if convert_process.returncode != 0:
                                        error_msg = convert_stderr.decode() if convert_stderr else "Unknown error"
                                        raise Exception(f"Failed to convert image to qcow2: {error_msg}")
                                    
                                    # Remove raw file
                                    os.remove(temp_path)
                                    logger.info(f"Successfully converted to qcow2 format")
                            else:
                                logger.warning(f"Could not check image format: {stderr.decode()}")
                
                download_progress[vm_name]['status'] = 'resizing'
                download_progress[vm_name]['message'] = 'Checking disk size...'
                logger.info(f"Checking current disk size for {disk_path}")
                
                # Get current disk size
                info_process = await asyncio.create_subprocess_exec(
                    'qemu-img', 'info', '--output=json', disk_path,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                info_stdout, info_stderr = await info_process.communicate()
                
                if info_process.returncode == 0:
                    import json as json_module
                    disk_info = json_module.loads(info_stdout.decode())
                    current_size_bytes = disk_info.get('virtual-size', 0)
                    current_size_gb = current_size_bytes / (1024**3)
                    target_size_gb = float(disk_size_gb)
                    
                    logger.info(f"Current disk size: {current_size_gb:.2f}GB, Target: {target_size_gb}GB")
                    
                    if current_size_gb < target_size_gb:
                        # Need to grow the disk
                        download_progress[vm_name]['message'] = f'Expanding disk from {current_size_gb:.1f}GB to {target_size_gb}GB...'
                        logger.info(f"Growing disk to {target_size_gb}GB")
                        
                        process = await asyncio.create_subprocess_exec(
                            'qemu-img', 'resize', disk_path, f'{disk_size_gb}G',
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE
                        )
                        stdout, stderr = await process.communicate()
                        
                        if process.returncode != 0:
                            error_msg = stderr.decode() if stderr else "Unknown error"
                            raise Exception(f"Failed to resize cloud image: {error_msg}")
                        
                        logger.info(f"Disk expanded successfully to {disk_size_gb}GB")
                    elif current_size_gb > target_size_gb:
                        # Image is already larger than requested - use current size
                        logger.info(f"Image is already {current_size_gb:.2f}GB, which is larger than requested {target_size_gb}GB. Using current size.")
                        download_progress[vm_name]['message'] = f'Using existing disk size ({current_size_gb:.1f}GB)'
                    else:
                        logger.info(f"Disk is already at target size {target_size_gb}GB")
                        download_progress[vm_name]['message'] = 'Disk size is correct'
                else:
                    # If we can't get info, try to resize anyway
                    logger.warning(f"Could not check disk size, attempting resize anyway")
                    download_progress[vm_name]['message'] = 'Resizing disk...'
                    
                    process = await asyncio.create_subprocess_exec(
                        'qemu-img', 'resize', disk_path, f'{disk_size_gb}G',
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    
                    if process.returncode != 0:
                        error_msg = stderr.decode() if stderr else "Unknown error"
                        # Don't fail if it's just a size issue
                        if "shrink" in error_msg.lower():
                            logger.warning(f"Image is larger than target size, using existing size")
                            download_progress[vm_name]['message'] = 'Using existing disk size'
                        else:
                            raise Exception(f"Failed to resize cloud image: {error_msg}")
                    
                    download_progress[vm_name]['status'] = 'complete'
                    download_progress[vm_name]['message'] = 'Image ready'
                    logger.info(f"Image preparation complete for {vm_name}")
                    
        else:
            # Create empty disk for ISO installation (VMs) or empty rootfs (LXC)
            if deploy_type == 'lxc':
                # Create empty rootfs directory for LXC
                rootfs_path = f"{DEFAULT_STORAGE_PATH}/{vm_name}-rootfs"
                os.makedirs(rootfs_path, exist_ok=True)
                disk_path = rootfs_path
                logger.info(f"Created empty rootfs directory at {rootfs_path}")
            else:
                # Create empty disk for VM
                disk_path = create_storage_volume(conn, vm_name, disk_size_gb)
            
    except Exception as e:
        if vm_name in download_progress:
            download_progress[vm_name]['status'] = 'error'
            download_progress[vm_name]['message'] = str(e)
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Disk creation failed: {e}'}, status=500)

    # Parse and modify the XML to inject the specific disk path and network configuration
    try:
        root = ET.fromstring(xml_config)
        
        # Ensure the name matches the new VM name
        name_element = root.find('name')
        if name_element is not None:
            name_element.text = vm_name
        
        # Handle disk/filesystem path injection based on type
        if deploy_type == 'lxc':
            # For LXC, update the filesystem source directory
            filesystem_element = root.find("./devices/filesystem[@type='mount']")
            if filesystem_element is not None:
                source_element = filesystem_element.find('source')
                if source_element is None:
                    source_element = ET.SubElement(filesystem_element, 'source')
                source_element.set('dir', disk_path)
                logger.info(f"Updated LXC rootfs path to {disk_path}")
            else:
                raise Exception("Remote LXC XML missing filesystem mount definition.")
            
            # Check and update init path if needed (for Alpine and other non-systemd distros)
            if cloud_image_url and cloud_image_url.endswith('.tar.xz'):
                # Detect init path in the extracted rootfs
                init_paths = ['/sbin/init', '/bin/init', '/lib/systemd/systemd', '/init']
                found_init = None
                for init_path in init_paths:
                    full_init_path = os.path.join(disk_path, init_path.lstrip('/'))
                    if os.path.exists(full_init_path):
                        found_init = init_path
                        break
                
                # Update init path in XML if we found one
                if found_init:
                    init_element = root.find('./os/init')
                    if init_element is not None:
                        init_element.text = found_init
                        logger.info(f"Updated LXC init path to {found_init}")
                    else:
                        # Add init element if not present
                        os_element = root.find('os')
                        if os_element is not None:
                            init_element = ET.SubElement(os_element, 'init')
                            init_element.text = found_init
                            logger.info(f"Added LXC init path: {found_init}")
        else:
            # For VMs, find the first disk that is a 'file' and is a 'disk' device
            disk_element = root.find("./devices/disk[@type='file'][@device='disk']")
            if disk_element is None:
                raise Exception("Remote VM XML missing a main disk definition to modify.")
            
            # Update the <source file='...'/> path
            source_element = disk_element.find('source')
            if source_element is None:
                source_element = ET.SubElement(disk_element, 'source')
            source_element.set('file', disk_path)
        
        # Update network interface to use current NETWORK_MODE
        devices = root.find('devices')
        if devices is not None:
            # Find and remove existing network interfaces
            for interface in devices.findall('interface'):
                devices.remove(interface)
            
            # Add new interface with correct network configuration
            if NETWORK_MODE == 'bridge':
                # Create bridge interface - points directly to bridge device
                interface = ET.SubElement(devices, 'interface', type='bridge')
                ET.SubElement(interface, 'source', bridge=BRIDGE_NAME)
                ET.SubElement(interface, 'model', type='virtio')
                logger.info(f"Configured VM {vm_name} with bridge device: {BRIDGE_NAME}")
            else:
                # Create NAT interface - uses libvirt network
                interface = ET.SubElement(devices, 'interface', type='network')
                ET.SubElement(interface, 'source', network=NAT_NETWORK_NAME)
                ET.SubElement(interface, 'model', type='virtio')
                logger.info(f"Configured VM {vm_name} with NAT network: {NAT_NETWORK_NAME}")
        
        # If ISO path provided, add CD-ROM device
        if iso_path and os.path.exists(iso_path):
            devices = root.find('devices')
            
            # Add CD-ROM disk
            cdrom = ET.SubElement(devices, 'disk', type='file', device='cdrom')
            ET.SubElement(cdrom, 'driver', name='qemu', type='raw')
            ET.SubElement(cdrom, 'source', file=iso_path)
            ET.SubElement(cdrom, 'target', dev='hdc', bus='sata')
            ET.SubElement(cdrom, 'readonly')
            
            # Update boot order to boot from CD first
            os_element = root.find('os')
            if os_element is not None:
                # Remove existing boot entries
                for boot in os_element.findall('boot'):
                    os_element.remove(boot)
                # Add CD boot first, then HD
                ET.SubElement(os_element, 'boot', dev='cdrom')
                ET.SubElement(os_element, 'boot', dev='hd')
        
        # Convert back to string
        final_xml = ET.tostring(root, encoding='unicode')
        
    except Exception as e:
        # Cleanup the disk if we fail XML modification
        logger.error(f"XML modification error for {vm_name}: {e}")
        if vm_name in download_progress:
            download_progress[vm_name]['status'] = 'error'
            download_progress[vm_name]['message'] = f'XML modification failed: {e}'
        try:
            if os.path.exists(disk_path):
                os.remove(disk_path)
        except:
            pass
        try:
            pool = conn.storagePoolLookupByName(DEFAULT_POOL_NAME)
            vol = pool.storageVolLookupByName(f"{vm_name}.qcow2")
            if vol: vol.delete(0)
        except:
            pass
        conn.close()
        return web.json_response({'status': 'error', 'message': f'XML modification error: {e}. Disk creation rolled back.'}, status=500)

    # 3. Define and Start the domain
    try:
        logger.info(f"Defining VM {vm_name} in libvirt...")
        domain = conn.defineXML(final_xml)
        if domain is None:
            raise Exception("Libvirt failed to define the domain from the modified XML.")
        
        logger.info(f"Starting VM {vm_name}...")
        domain.create()
        
        # Store metadata with type information
        if deploy_type == 'lxc':
            set_lxc_metadata(vm_name, {'type': 'lxc', 'description': f'LXC container: {vm_name}'})
        else:
            set_vm_metadata(vm_name, {'type': 'vm'})
        
        # Clean up download progress on success
        if vm_name in download_progress:
            del download_progress[vm_name]
        
        conn.close()
        entity_type = "Container" if deploy_type == 'lxc' else "VM"
        logger.info(f"{entity_type} {vm_name} successfully deployed and started")
        
        if cloud_image_url:
            return web.json_response({'status': 'success', 'message': f'{entity_type} {vm_name} deployed from cloud image and started with a {disk_size_gb}GB disk.'})
        elif iso_path:
            return web.json_response({'status': 'success', 'message': f'{entity_type} {vm_name} created with installation ISO. Complete installation via VNC console.'})
        else:
            return web.json_response({'status': 'success', 'message': f'{entity_type} {vm_name} deployed with empty disk. Attach an ISO or image to install an OS.'})
        
    except libvirt.libvirtError as e:
        # Better error handling - don't try to delete things that might not exist
        error_message = str(e)
        logger.error(f"Libvirt error for {vm_name}: {error_message}")
        if vm_name in download_progress:
            download_progress[vm_name]['status'] = 'error'
            download_progress[vm_name]['message'] = f'VM creation failed: {error_message}'
        conn.close()
        return web.json_response({'status': 'error', 'message': f'Libvirt operation failed during deployment: {error_message}'}, status=500)
    except Exception as e:
        logger.error(f"Unexpected error for {vm_name}: {e}")
        if vm_name in download_progress:
            download_progress[vm_name]['status'] = 'error'
            download_progress[vm_name]['message'] = str(e)
        conn.close()
        return web.json_response({'status': 'error', 'message': str(e)}, status=500)


# --- Repository Management API Handlers ---

async def list_repositories(request):
    """Returns the list of configured repositories."""
    config = load_repositories_config()
    return web.json_response({'status': 'success', 'repositories': config.get('repositories', [])})

async def get_all_apps(request):
    """Fetches and aggregates apps from all enabled repositories."""
    config = load_repositories_config()
    all_apps = []
    
    # Fetch apps from all enabled repositories
    for repo in config.get('repositories', []):
        if repo.get('enabled', False):
            apps = await fetch_repository_apps(repo['url'])
            if apps:
                # Add repository metadata to each app
                for app in apps:
                    app['repo_id'] = repo['id']
                    app['repo_name'] = repo['name']
                all_apps.extend(apps)
    
    return web.json_response({'status': 'success', 'apps': all_apps, 'total': len(all_apps)})

async def add_repository(request):
    """Adds a new repository to the configuration."""
    try:
        data = await request.json()
    except json.JSONDecodeError:
        return web.json_response({'status': 'error', 'message': 'Invalid JSON body.'}, status=400)
    
    repo_id = data.get('id')
    name = data.get('name')
    url = data.get('url')
    
    if not all([repo_id, name, url]):
        return web.json_response({'status': 'error', 'message': 'Missing required fields: id, name, url'}, status=400)
    
    config = load_repositories_config()
    
    # Check if repo_id already exists
    for repo in config['repositories']:
        if repo['id'] == repo_id:
            return web.json_response({'status': 'error', 'message': f'Repository with id {repo_id} already exists.'}, status=409)
    
    # Add new repository
    new_repo = {
        'id': repo_id,
        'name': name,
        'url': url,
        'enabled': data.get('enabled', True),
        'description': data.get('description', '')
    }
    config['repositories'].append(new_repo)
    
    if save_repositories_config(config):
        return web.json_response({'status': 'success', 'message': f'Repository {name} added successfully.', 'repository': new_repo})
    else:
        return web.json_response({'status': 'error', 'message': 'Failed to save configuration.'}, status=500)

async def update_repository(request):
    """Updates an existing repository (enable/disable, change URL, etc.)."""
    repo_id = request.match_info.get('id')
    
    try:
        data = await request.json()
    except json.JSONDecodeError:
        return web.json_response({'status': 'error', 'message': 'Invalid JSON body.'}, status=400)
    
    config = load_repositories_config()
    
    # Find and update the repository
    for repo in config['repositories']:
        if repo['id'] == repo_id:
            if 'name' in data:
                repo['name'] = data['name']
            if 'url' in data:
                repo['url'] = data['url']
            if 'enabled' in data:
                repo['enabled'] = data['enabled']
            if 'description' in data:
                repo['description'] = data['description']
            
            if save_repositories_config(config):
                return web.json_response({'status': 'success', 'message': f'Repository {repo_id} updated successfully.', 'repository': repo})
            else:
                return web.json_response({'status': 'error', 'message': 'Failed to save configuration.'}, status=500)
    
    return web.json_response({'status': 'error', 'message': f'Repository {repo_id} not found.'}, status=404)

async def delete_repository(request):
    """Removes a repository from the configuration."""
    repo_id = request.match_info.get('id')
    
    config = load_repositories_config()
    
    # Find and remove the repository
    original_length = len(config['repositories'])
    config['repositories'] = [repo for repo in config['repositories'] if repo['id'] != repo_id]
    
    if len(config['repositories']) == original_length:
        return web.json_response({'status': 'error', 'message': f'Repository {repo_id} not found.'}, status=404)
    
    if save_repositories_config(config):
        return web.json_response({'status': 'success', 'message': f'Repository {repo_id} removed successfully.'})
    else:
        return web.json_response({'status': 'error', 'message': 'Failed to save configuration.'}, status=500)

async def get_download_progress(request):
    """Returns the current download progress for a VM."""
    vm_name = request.match_info.get('vm_name')
    
    if vm_name in download_progress:
        return web.json_response({'status': 'success', 'progress': download_progress[vm_name]})
    else:
        return web.json_response({'status': 'error', 'message': 'No download in progress for this VM'}, status=404)

async def get_all_downloads(request):
    """Returns all current download progresses."""
    return web.json_response({'status': 'success', 'downloads': download_progress})

async def get_deployment_logs(request):
    """Returns recent deployment logs for debugging."""
    # Return last 100 lines of logs (stored in memory)
    # For now, just return download progress and any stored errors
    return web.json_response({
        'status': 'success', 
        'downloads': download_progress,
        'message': 'Check server console for detailed logs'
    })

async def cleanup_orphaned_containers(request):
    """Finds and optionally removes orphaned LXC containers that failed during installation."""
    try:
        data = await request.json()
    except json.JSONDecodeError:
        data = {}
    
    action = data.get('action', 'list')  # 'list' or 'cleanup'
    container_name = data.get('name')  # Optional: specific container to cleanup
    
    orphaned = []
    
    # Check LXC connection for containers
    lxc_conn = get_connection('lxc')
    if lxc_conn:
        try:
            containers = lxc_conn.listAllDomains(libvirt.VIR_CONNECT_LIST_DOMAINS_ACTIVE | libvirt.VIR_CONNECT_LIST_DOMAINS_INACTIVE)
            
            for container in containers:
                name = container.name()
                
                # Check if this container has metadata (properly installed)
                lxc_metadata = get_lxc_metadata(name)
                if not lxc_metadata or 'type' not in lxc_metadata:
                    # This is an orphaned container
                    orphaned.append({
                        'name': name,
                        'state': container.isActive(),
                        'id': container.ID() if container.isActive() else None
                    })
                    
                    # If cleanup action and matches the name (or no specific name given)
                    if action == 'cleanup' and (not container_name or container_name == name):
                        try:
                            # Stop if running
                            if container.isActive():
                                container.destroy()
                                logger.info(f"Stopped orphaned container {name}")
                            
                            # Undefine
                            container.undefine()
                            logger.info(f"Undefined orphaned container {name}")
                            
                            # Try to delete rootfs
                            rootfs_path = f"{DEFAULT_STORAGE_PATH}/{name}-rootfs"
                            if os.path.exists(rootfs_path):
                                shutil.rmtree(rootfs_path)
                                logger.info(f"Deleted orphaned rootfs at {rootfs_path}")
                        except Exception as e:
                            logger.error(f"Failed to cleanup {name}: {e}")
            
            lxc_conn.close()
        except Exception as e:
            logger.error(f"Error checking LXC containers: {e}")
            if lxc_conn:
                lxc_conn.close()
    
    if action == 'cleanup':
        return web.json_response({
            'status': 'success',
            'message': f'Cleaned up {len(orphaned)} orphaned container(s)',
            'cleaned': orphaned
        })
    else:
        return web.json_response({
            'status': 'success',
            'orphaned_containers': orphaned
        })

# --- Updater System Functions ---

def load_update_config():
    """Loads update configuration from JSON file."""
    try:
        if not os.path.exists(UPDATE_CONFIG_PATH):
            # Default configuration
            return {
                'auto_update_enabled': False,
                'update_channel': 'main',
                'last_check': None,
                'last_update': None
            }
        with open(UPDATE_CONFIG_PATH, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading update config: {e}")
        return {
            'auto_update_enabled': False,
            'update_channel': 'main',
            'last_check': None,
            'last_update': None
        }

def save_update_config(config):
    """Saves update configuration to JSON file."""
    try:
        os.makedirs(os.path.dirname(UPDATE_CONFIG_PATH), exist_ok=True)
        with open(UPDATE_CONFIG_PATH, 'w') as f:
            json.dump(config, f, indent=2)
        return True
    except Exception as e:
        logger.error(f"Error saving update config: {e}")
        return False

def validate_git_repo():
    """Validates that GIT_REPO_PATH points to a valid git repository."""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', '--git-dir'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except Exception:
        return False

def get_current_version():
    """Gets the current version information."""
    try:
        # Try to get version from version file
        if os.path.exists(VERSION_FILE_PATH):
            with open(VERSION_FILE_PATH, 'r') as f:
                return json.load(f)
        
        # Fall back to git if no version file exists
        if not validate_git_repo():
            return {
                'version': 'unknown',
                'commit': 'unknown',
                'date': 'unknown',
                'branch': 'unknown'
            }
        
        result = subprocess.run(
            ['git', 'rev-parse', '--short', 'HEAD'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            commit_hash = result.stdout.strip()
            
            # Get commit date
            date_result = subprocess.run(
                ['git', 'log', '-1', '--format=%ci'],
                cwd=GIT_REPO_PATH,
                capture_output=True,
                text=True,
                timeout=5
            )
            commit_date = date_result.stdout.strip() if date_result.returncode == 0 else 'unknown'
            
            return {
                'version': commit_hash,
                'commit': commit_hash,
                'date': commit_date,
                'branch': 'main'
            }
        else:
            return {
                'version': 'unknown',
                'commit': 'unknown',
                'date': 'unknown',
                'branch': 'unknown'
            }
    except Exception as e:
        logger.error(f"Error getting current version: {e}")
        return {
            'version': 'unknown',
            'commit': 'unknown',
            'date': 'unknown',
            'branch': 'unknown'
        }

def check_for_updates():
    """Checks if updates are available."""
    try:
        # Validate git repository first
        if not validate_git_repo():
            logger.error("GIT_REPO_PATH does not point to a valid git repository")
            return None
        
        # Fetch latest changes from remote
        result = subprocess.run(
            ['git', 'fetch', 'origin'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode != 0:
            logger.error(f"Failed to fetch updates: {result.stderr}")
            return None
        
        # Check if there are new commits
        result = subprocess.run(
            ['git', 'rev-list', '--count', 'HEAD..origin/main'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            commit_count = int(result.stdout.strip())
            
            if commit_count > 0:
                # Get the latest commit info
                log_result = subprocess.run(
                    ['git', 'log', 'origin/main', '-1', '--format=%H|%ci|%s'],
                    cwd=GIT_REPO_PATH,
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                
                if log_result.returncode == 0:
                    parts = log_result.stdout.strip().split('|')
                    return {
                        'available': True,
                        'commit_count': commit_count,
                        'latest_commit': parts[0] if len(parts) > 0 else 'unknown',
                        'latest_date': parts[1] if len(parts) > 1 else 'unknown',
                        'latest_message': parts[2] if len(parts) > 2 else 'Update available'
                    }
            
            return {
                'available': False,
                'commit_count': 0,
                'message': 'System is up to date'
            }
        else:
            logger.error(f"Failed to check for updates: {result.stderr}")
            return None
    except Exception as e:
        logger.error(f"Error checking for updates: {e}")
        return None

def create_backup():
    """Creates a backup of the current installation."""
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = os.path.join(BACKUP_DIR, f'backup_{timestamp}')
        
        os.makedirs(BACKUP_DIR, exist_ok=True)
        
        # Get current commit hash
        result = subprocess.run(
            ['git', 'rev-parse', 'HEAD'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=5
        )
        
        commit_hash = result.stdout.strip() if result.returncode == 0 else 'unknown'
        
        # Save backup metadata
        backup_info = {
            'timestamp': timestamp,
            'commit': commit_hash,
            'path': backup_path
        }
        
        backup_info_path = os.path.join(BACKUP_DIR, f'backup_{timestamp}.json')
        with open(backup_info_path, 'w') as f:
            json.dump(backup_info, f, indent=2)
        
        logger.info(f"Created backup metadata at {backup_info_path}")
        
        return backup_info
    except Exception as e:
        logger.error(f"Error creating backup: {e}")
        return None

def perform_update():
    """Performs the system update."""
    try:
        # Create backup first
        backup_info = create_backup()
        if not backup_info:
            return {'success': False, 'error': 'Failed to create backup'}
        
        logger.info("Starting update process...")
        
        # Stash any local changes (ignore errors if nothing to stash)
        stash_result = subprocess.run(
            ['git', 'stash', 'push', '-m', 'Auto-stash before update'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            timeout=10
        )
        if stash_result.returncode != 0:
            logger.debug(f"Git stash returned non-zero: {stash_result.stderr}")
        
        # Pull latest changes
        result = subprocess.run(
            ['git', 'pull', 'origin', 'main'],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode != 0:
            logger.error(f"Update failed: {result.stderr}")
            # Attempt rollback
            rollback_result = rollback_update(backup_info['commit'])
            return {
                'success': False,
                'error': f'Update failed: {result.stderr}',
                'rollback_attempted': True,
                'rollback_success': rollback_result['success'] if rollback_result else False
            }
        
        logger.info("Update completed successfully")
        
        # Update the last update timestamp
        config = load_update_config()
        config['last_update'] = datetime.now().isoformat()
        save_update_config(config)
        
        # Schedule service restart
        return {
            'success': True,
            'message': 'Update completed successfully',
            'backup': backup_info,
            'restart_required': True
        }
    except Exception as e:
        logger.error(f"Error performing update: {e}")
        return {'success': False, 'error': str(e)}

def schedule_service_restart(delay_seconds=2):
    """Schedules a service restart after a delay."""
    async def restart_service():
        await asyncio.sleep(delay_seconds)
        try:
            subprocess.run(
                ['systemctl', 'restart', SERVICE_NAME],
                timeout=10
            )
            logger.info("Service restart triggered")
        except Exception as e:
            logger.error(f"Failed to restart service: {e}")
    
    asyncio.create_task(restart_service())

def rollback_update(commit_hash):
    """Rolls back to a previous commit."""
    try:
        logger.info(f"Rolling back to commit {commit_hash}")
        
        # Reset to the specified commit
        result = subprocess.run(
            ['git', 'reset', '--hard', commit_hash],
            cwd=GIT_REPO_PATH,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode != 0:
            logger.error(f"Rollback failed: {result.stderr}")
            return {'success': False, 'error': result.stderr}
        
        logger.info("Rollback completed successfully")
        return {
            'success': True,
            'message': f'Rolled back to commit {commit_hash}'
        }
    except Exception as e:
        logger.error(f"Error during rollback: {e}")
        return {'success': False, 'error': str(e)}

def perform_health_check():
    """Performs a health check of the system after update."""
    try:
        # Check if the backend service is running
        result = subprocess.run(
            ['systemctl', 'is-active', SERVICE_NAME],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        service_running = result.returncode == 0 and result.stdout.strip() == 'active'
        
        # Check if we can connect to libvirt
        libvirt_ok = get_connection() is not None
        
        return {
            'healthy': service_running and libvirt_ok,
            'service_running': service_running,
            'libvirt_connected': libvirt_ok
        }
    except Exception as e:
        logger.error(f"Error during health check: {e}")
        return {
            'healthy': False,
            'error': str(e)
        }

async def get_update_status(request):
    """Returns the current update status and configuration."""
    try:
        config = load_update_config()
        current_version = get_current_version()
        
        # Update last check timestamp
        config['last_check'] = datetime.now().isoformat()
        save_update_config(config)
        
        return web.json_response({
            'status': 'success',
            'current_version': current_version,
            'config': config
        })
    except Exception as e:
        logger.error(f"Error getting update status: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

async def check_updates(request):
    """Checks for available updates."""
    try:
        update_info = check_for_updates()
        
        if update_info is None:
            return web.json_response({
                'status': 'error',
                'message': 'Failed to check for updates'
            }, status=500)
        
        # Update last check timestamp
        config = load_update_config()
        config['last_check'] = datetime.now().isoformat()
        save_update_config(config)
        
        return web.json_response({
            'status': 'success',
            'update_info': update_info
        })
    except Exception as e:
        logger.error(f"Error checking for updates: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

async def trigger_update(request):
    """Triggers the update process."""
    try:
        # Perform the update
        result = perform_update()
        
        if result['success']:
            # If update was successful and restart is required, schedule restart
            if result.get('restart_required'):
                schedule_service_restart()
            
            return web.json_response({
                'status': 'success',
                'message': result['message'],
                'restart_required': result.get('restart_required', False)
            })
        else:
            return web.json_response({
                'status': 'error',
                'message': result.get('error', 'Update failed'),
                'rollback_attempted': result.get('rollback_attempted', False),
                'rollback_success': result.get('rollback_success', False)
            }, status=500)
    except Exception as e:
        logger.error(f"Error triggering update: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

async def update_auto_update_config(request):
    """Updates the auto-update configuration."""
    try:
        data = await request.json()
        
        config = load_update_config()
        
        if 'auto_update_enabled' in data:
            config['auto_update_enabled'] = bool(data['auto_update_enabled'])
        
        if 'update_channel' in data:
            config['update_channel'] = data['update_channel']
        
        if save_update_config(config):
            return web.json_response({
                'status': 'success',
                'message': 'Configuration updated successfully',
                'config': config
            })
        else:
            return web.json_response({
                'status': 'error',
                'message': 'Failed to save configuration'
            }, status=500)
    except Exception as e:
        logger.error(f"Error updating auto-update config: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

async def get_update_history(request):
    """Returns the update history from backups."""
    try:
        if not os.path.exists(BACKUP_DIR):
            return web.json_response({
                'status': 'success',
                'backups': []
            })
        
        backups = []
        for filename in os.listdir(BACKUP_DIR):
            if filename.endswith('.json'):
                try:
                    with open(os.path.join(BACKUP_DIR, filename), 'r') as f:
                        backup_info = json.load(f)
                        backups.append(backup_info)
                except Exception as e:
                    logger.error(f"Error reading backup file {filename}: {e}")
        
        # Sort by timestamp, newest first
        backups.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return web.json_response({
            'status': 'success',
            'backups': backups
        })
    except Exception as e:
        logger.error(f"Error getting update history: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

async def perform_rollback(request):
    """Performs a rollback to a previous version."""
    try:
        data = await request.json()
        commit_hash = data.get('commit')
        
        if not commit_hash:
            return web.json_response({
                'status': 'error',
                'message': 'Commit hash is required'
            }, status=400)
        
        result = rollback_update(commit_hash)
        
        if result['success']:
            # Schedule service restart
            schedule_service_restart()
            
            return web.json_response({
                'status': 'success',
                'message': result['message'],
                'restart_required': True
            })
        else:
            return web.json_response({
                'status': 'error',
                'message': result.get('error', 'Rollback failed')
            }, status=500)
    except Exception as e:
        logger.error(f"Error performing rollback: {e}")
        return web.json_response({
            'status': 'error',
            'message': str(e)
        }, status=500)

# --- CRITICAL FIX: VNC WebSocket Proxy Handler (Unchanged) ---

async def vnc_proxy_handler(request):
    """
    Handles the WebSocket connection from the browser and proxies raw TCP data 
    to the QEMU VNC server on localhost. This bridges the protocols.
    """
    vnc_port = request.match_info.get('port')
    
    if not vnc_port or not vnc_port.isdigit():
        return web.Response(status=400, text="Invalid VNC port specified.")

    vnc_port = int(vnc_port)
    vnc_host = '127.0.0.1'
    
    # 1. Initiate WebSocket connection with the client (browser)
    ws_client = web.WebSocketResponse()
    await ws_client.prepare(request)
    logger.debug(f"VNC: WebSocket connection opened for port {vnc_port}.")

    # 2. Open raw TCP socket connection to the local QEMU VNC server
    try:
        qemu_reader, qemu_writer = await asyncio.open_connection(vnc_host, vnc_port)
        logger.debug(f"VNC: TCP connection established to QEMU VNC at {vnc_host}:{vnc_port}.")
    except ConnectionRefusedError:
        logger.error(f"VNC: Connection refused to QEMU VNC at {vnc_host}:{vnc_port}. Is the VM running?")
        await ws_client.close(code=1006, message=b"Connection refused by VNC server (QEMU).")
        return ws_client
    except Exception as e:
        logger.error(f"VNC: Failed to establish TCP connection to QEMU: {e}")
        await ws_client.close(code=1006, message=b"Internal proxy error.")
        return ws_client

    # 3. Create concurrent tasks for two-way proxying
    
    bytes_sent = 0
    bytes_received = 0
    
    # Task 1: Read from QEMU (TCP) and send to Client (WebSocket)
    async def qemu_to_client():
        nonlocal bytes_sent
        try:
            while not ws_client.closed:
                # Read raw data from QEMU VNC server
                data = await qemu_reader.read(8192)  # Increased buffer size
                if not data:
                    logger.debug("VNC: QEMU closed the TCP connection.")
                    break
                
                bytes_sent += len(data)
                if bytes_sent <= 100:  # Log first few bytes for debugging
                    logger.debug(f"VNC: Sending {len(data)} bytes to client (total: {bytes_sent})")
                
                # Send the raw data through the WebSocket as binary
                await ws_client.send_bytes(data)
                
        except asyncio.CancelledError:
            logger.debug("VNC: qemu_to_client task cancelled.")
        except Exception as e:
            logger.error(f"VNC: qemu_to_client task error: {e}")
        finally:
            logger.debug(f"VNC: qemu_to_client task ended. Total bytes sent: {bytes_sent}")

    # Task 2: Read from Client (WebSocket) and send to QEMU (TCP)
    async def client_to_qemu():
        nonlocal bytes_received
        try:
            async for msg in ws_client:
                if msg.type == web.WSMsgType.BINARY:
                    # Received binary data from the client, write it to QEMU's TCP socket
                    bytes_received += len(msg.data)
                    if bytes_received <= 100:  # Log first few bytes for debugging
                        logger.debug(f"VNC: Received {len(msg.data)} bytes from client (total: {bytes_received})")
                    
                    qemu_writer.write(msg.data)
                    await qemu_writer.drain()
                    
                elif msg.type == web.WSMsgType.TEXT:
                    logger.warning(f"VNC: Received text message (expected binary): {msg.data}")
                    
                elif msg.type == web.WSMsgType.CLOSE:
                    logger.debug("VNC: Client sent close frame.")
                    break
                    
        except asyncio.CancelledError:
            logger.debug("VNC: client_to_qemu task cancelled.")
        except Exception as e:
            logger.error(f"VNC: client_to_qemu task error: {e}")
        finally:
            logger.debug(f"VNC: client_to_qemu task ended. Total bytes received: {bytes_received}")
    
    # Run the two tasks concurrently until one of them finishes
    qemu_task = asyncio.create_task(qemu_to_client())
    client_task = asyncio.create_task(client_to_qemu())

    try:
        # Wait for either the client or the QEMU side to close the connection
        done, pending = await asyncio.wait(
            [qemu_task, client_task],
            return_when=asyncio.FIRST_COMPLETED
        )

        # Cancel the remaining task
        for task in pending:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
                
    except Exception as e:
        logger.error(f"VNC: Exception in main proxy loop: {e}")
    finally:
        # Close connections
        try:
            qemu_writer.close()
            await qemu_writer.wait_closed()
        except Exception as e:
            logger.warning(f"VNC: Error closing QEMU writer: {e}")
            
        if not ws_client.closed:
            await ws_client.close()

    logger.debug(f"VNC: Proxy finished for port {vnc_port}. Sent: {bytes_sent}, Received: {bytes_received}")
    return ws_client


# --- LXC Console WebSocket Proxy Handler ---

async def lxc_console_handler(request):
    """
    Handles the WebSocket connection from the browser and proxies to the LXC console
    via nsenter command with PTY. This bridges the WebSocket protocol to the LXC console.
    """
    container_name = request.match_info.get('name')
    
    if not container_name:
        return web.Response(status=400, text="Invalid container name specified.")
    
    # 1. Initiate WebSocket connection with the client (browser)
    ws_client = web.WebSocketResponse()
    await ws_client.prepare(request)
    logger.info(f"LXC Console: WebSocket connection opened for container {container_name}.")
    
    # 2. Get container PID
    try:
        # First, get the container's init PID
        conn = get_connection('lxc')
        if not conn:
            error_msg = "Failed to connect to LXC driver"
            logger.error(f"LXC Console: {error_msg}")
            await ws_client.send_str(f"Error: {error_msg}\r\n")
            await ws_client.close(code=1006, message=b"Failed to connect to LXC.")
            return ws_client
        
        domain = get_domain_by_name(conn, container_name)
        if not domain:
            error_msg = f"Container {container_name} not found"
            logger.error(f"LXC Console: {error_msg}")
            conn.close()
            await ws_client.send_str(f"Error: {error_msg}\r\n")
            await ws_client.close(code=1006, message=b"Container not found.")
            return ws_client
        
        if not domain.isActive():
            error_msg = f"Container {container_name} is not running"
            logger.error(f"LXC Console: {error_msg}")
            conn.close()
            await ws_client.send_str(f"Error: {error_msg}\r\n")
            await ws_client.close(code=1006, message=b"Container not running.")
            return ws_client
        
        # Get the domain ID first
        domain_id = domain.ID()
        
        # Get the actual container init PID from XML or cgroup
        # For LXC, domain.ID() may not be reliable, so we parse the domain XML
        xml_desc = domain.XMLDesc(0)
        root = ET.fromstring(xml_desc)
        
        # Try to find PID from domain XML metadata
        init_pid = None
        
        # Method 1: Check if there's a PID in the domain's running state
        # For LXC, we need to find the actual init process
        # We'll search for it in /sys/fs/cgroup or use virsh commands
        
        conn.close()
        
        # Method 2: Find the container's init PID from systemd-machined
        try:
            # Get the machine name from libvirt - it's lxc-<domain_id>-<name>
            machine_name = f"lxc-{domain_id}-{container_name}"
            logger.info(f"LXC Console: Attempting machinectl with machine name: {machine_name}")
            
            # Use machinectl to get the leader PID
            process_info = await asyncio.create_subprocess_exec(
                'machinectl', 'show', '-p', 'Leader', machine_name,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process_info.communicate()
            
            logger.info(f"LXC Console: machinectl returned code {process_info.returncode}, stdout: {stdout.decode()}, stderr: {stderr.decode()}")
            
            if process_info.returncode == 0:
                output = stdout.decode('utf-8').strip()
                # Output format: "Leader=12345"
                if 'Leader=' in output:
                    init_pid = int(output.split('=')[1])
                    logger.info(f"LXC Console: Found container init PID via machinectl: {init_pid}")
                else:
                    logger.warning(f"LXC Console: machinectl succeeded but no Leader= in output: {output}")
            else:
                logger.warning(f"LXC Console: machinectl failed with return code {process_info.returncode}")
        except Exception as e:
            logger.warning(f"LXC Console: Could not get PID from machinectl: {e}")
        
        # Method 3: Fallback - search for the init process in cgroup
        if not init_pid:
            try:
                # Look for the container's cgroup and find init PID
                # Path format: /sys/fs/cgroup/machine.slice/machine-lxc\x2d{domain_id}\x2d{container_name}.scope/cgroup.procs
                cgroup_path = f"/sys/fs/cgroup/machine.slice/machine-lxc\\x2d{domain_id}\\x2d{container_name}.scope/cgroup.procs"
                if os.path.exists(cgroup_path):
                    with open(cgroup_path, 'r') as f:
                        pids = f.read().strip().split('\n')
                        if pids and pids[0]:
                            init_pid = int(pids[0])  # First PID should be init
                            logger.info(f"LXC Console: Found container init PID from cgroup: {init_pid}")
            except Exception as e:
                logger.warning(f"LXC Console: Could not get PID from cgroup: {e}")
        
        # Method 4: Last resort - use domain ID
        if not init_pid:
            init_pid = domain_id
            logger.warning(f"LXC Console: Using domain ID as PID (may not be correct): {init_pid}")
        
        if not init_pid or init_pid <= 0:
            error_msg = f"Could not get PID for container {container_name}"
            logger.error(f"LXC Console: {error_msg}")
            await ws_client.send_str(f"Error: {error_msg}\r\n")
            await ws_client.close(code=1006, message=b"Could not get container PID.")
            return ws_client
        
        logger.info(f"LXC Console: Container {container_name} will use PID: {init_pid}")
        
    except Exception as e:
        logger.error(f"LXC Console: Failed to get container info: {e}")
        await ws_client.send_str(f"Error: {e}\r\n")
        await ws_client.close(code=1006, message=b"Failed to get container info.")
        return ws_client
    
    # 3. Create PTY and start nsenter with it
    master_fd = None
    process = None
    
    try:
        # Create a PTY pair
        master_fd, slave_fd = pty.openpty()
        
        # Set master to non-blocking mode
        flags = fcntl.fcntl(master_fd, fcntl.F_GETFL)
        fcntl.fcntl(master_fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)
        
        # Use script command to wrap nsenter - this properly handles PTY allocation
        # script -qfc runs a command with a PTY and exits immediately
        # We set the environment variables that /bin/sh needs to work properly
        cmd = f"script -qfc 'nsenter -t {init_pid} -a env TERM=xterm-256color PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOME=/root PS1=\"/ # \" /bin/sh' /dev/null"
        
        process = await asyncio.create_subprocess_shell(
            cmd,
            stdin=slave_fd,
            stdout=slave_fd,
            stderr=slave_fd
        )
        
        # Close slave_fd in parent - child has its own copy
        os.close(slave_fd)
        
        logger.info(f"LXC Console: Started nsenter shell for {container_name} (PID: {process.pid})")
        
    except Exception as e:
        logger.error(f"LXC Console: Failed to create PTY and start nsenter: {e}")
        if master_fd is not None:
            try:
                os.close(master_fd)
            except:
                pass
        await ws_client.send_str(f"Error: Failed to start console: {e}\r\n")
        await ws_client.close(code=1006, message=b"Failed to start console.")
        return ws_client
    
    bytes_sent = 0
    bytes_received = 0
    
    # Task 1: Read from PTY and send to Client (WebSocket)
    async def pty_to_client():
        nonlocal bytes_sent
        try:
            while not ws_client.closed:
                # Read data from PTY (non-blocking)
                try:
                    data = os.read(master_fd, 8192)
                    if not data:
                        logger.debug("LXC Console: PTY closed.")
                        break
                    
                    bytes_sent += len(data)
                    
                    # Send the data through the WebSocket as text
                    try:
                        await ws_client.send_str(data.decode('utf-8', errors='replace'))
                    except Exception as e:
                        logger.error(f"LXC Console: Error sending data to client: {e}")
                        break
                        
                except BlockingIOError:
                    # No data available, sleep briefly
                    await asyncio.sleep(0.01)
                except OSError as e:
                    logger.debug(f"LXC Console: PTY read error (process may have exited): {e}")
                    break
                
        except asyncio.CancelledError:
            logger.debug("LXC Console: pty_to_client task cancelled.")
        except Exception as e:
            logger.error(f"LXC Console: pty_to_client task error: {e}")
        finally:
            logger.debug(f"LXC Console: pty_to_client task ended. Total bytes sent: {bytes_sent}")
    
    # Task 2: Read from Client (WebSocket) and write to PTY
    async def client_to_pty():
        nonlocal bytes_received
        try:
            async for msg in ws_client:
                if msg.type == web.WSMsgType.TEXT:
                    # Received text data from the client, write it to PTY
                    data = msg.data.encode('utf-8')
                    bytes_received += len(data)
                    
                    try:
                        os.write(master_fd, data)
                    except Exception as e:
                        logger.error(f"LXC Console: Error writing to PTY: {e}")
                        break
                    
                elif msg.type == web.WSMsgType.BINARY:
                    logger.warning(f"LXC Console: Received binary message (expected text)")
                    
                elif msg.type == web.WSMsgType.CLOSE:
                    logger.debug("LXC Console: Client sent close frame.")
                    break
                    
        except asyncio.CancelledError:
            logger.debug("LXC Console: client_to_pty task cancelled.")
        except Exception as e:
            logger.error(f"LXC Console: client_to_pty task error: {e}")
        finally:
            logger.debug(f"LXC Console: client_to_pty task ended. Total bytes received: {bytes_received}")
    
    # Run the two tasks concurrently until one of them finishes
    pty_task = asyncio.create_task(pty_to_client())
    client_task = asyncio.create_task(client_to_pty())
    
    try:
        # Wait for either the client or the PTY side to close the connection
        done, pending = await asyncio.wait(
            [pty_task, client_task],
            return_when=asyncio.FIRST_COMPLETED
        )
        
        # Cancel the remaining task
        for task in pending:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
                
    except Exception as e:
        logger.error(f"LXC Console: Exception in main proxy loop: {e}")
    finally:
        # Close connections
        try:
            if master_fd is not None:
                os.close(master_fd)
        except Exception as e:
            logger.warning(f"LXC Console: Error closing PTY: {e}")
        
        try:
            if process is not None:
                process.terminate()
                try:
                    await asyncio.wait_for(process.wait(), timeout=2.0)
                except asyncio.TimeoutError:
                    process.kill()
                    await process.wait()
        except Exception as e:
            logger.warning(f"LXC Console: Error terminating process: {e}")
            
        if not ws_client.closed:
            await ws_client.close()
    
    logger.debug(f"LXC Console: Proxy finished for {container_name}. Sent: {bytes_sent}, Received: {bytes_received}")
    return ws_client


# --- Application Setup ---

def init_app():
    """Initializes the Aiohttp application with routes."""
    app = web.Application()
    
    # Set up CORS middleware to allow the NGINX frontend to access the API
    async def cors_middleware(app, handler):
        async def middleware_handler(request):
            response = await handler(request)
            response.headers['Access-Control-Allow-Origin'] = '*'
            response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'
            response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
            return response
        return middleware_handler

    # Add the CORS middleware (Note: Aiohttp middleware is applied in reverse order)
    app.middlewares.append(cors_middleware) 

    # ---< API Routes >---
    # VM Management
    app.router.add_get('/api/vm_list', list_vms)
    app.router.add_post('/api/vm', create_vm)
    app.router.add_post('/api/vm/deploy', deploy_vm_from_url)
    app.router.add_get('/api/vm/{name}/disk-info', get_vm_disk_info)
    app.router.add_put('/api/vm/{name}/settings', update_vm_settings)
    app.router.add_post('/api/vm/{name}/{action}', vm_action)
    # Repository Management
    app.router.add_get('/api/repositories', list_repositories)
    app.router.add_get('/api/repositories/apps', get_all_apps)
    app.router.add_post('/api/repositories', add_repository)
    app.router.add_put('/api/repositories/{id}', update_repository)
    app.router.add_delete('/api/repositories/{id}', delete_repository)
    # Download Progress
    app.router.add_get('/api/downloads/{vm_name}', get_download_progress)
    app.router.add_get('/api/downloads', get_all_downloads)
    app.router.add_get('/api/logs', get_deployment_logs)
    # Cleanup
    app.router.add_post('/api/cleanup/orphaned', cleanup_orphaned_containers)
    # Updater
    app.router.add_get('/api/update/status', get_update_status)
    app.router.add_get('/api/update/check', check_updates)
    app.router.add_post('/api/update/trigger', trigger_update)
    app.router.add_post('/api/update/rollback', perform_rollback)
    app.router.add_put('/api/update/config', update_auto_update_config)
    app.router.add_get('/api/update/history', get_update_history)

    # ---< Proxies >---
    # VNC 
    app.router.add_get('/vnc-proxy/{port}', vnc_proxy_handler)
    # LXC Console
    app.router.add_get('/lxc-console/{name}', lxc_console_handler)
    
    return app

if __name__ == '__main__':
    # Log connection attempt status
    if get_connection():
        logger.info("Successfully connected to libvirt. Aiohttp API & WS Proxy ready.")
    else:
        logger.critical("Failed to connect to libvirt. Ensure KVM/libvirt is installed and service is running.")

    # Start the Aiohttp application
    app = init_app()
    web.run_app(app, host='127.0.0.1', port=5000)
